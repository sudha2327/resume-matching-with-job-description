{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593901c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('C:/Users/sudha/job decr/jb_df.csv')  # <-- update path if needed\n",
    "df['Job Description'] = df['Job Description'].fillna('')\n",
    "\n",
    "# Preprocessing Function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# Clean Descriptions\n",
    "df['clean'] = df['Job Description'].apply(preprocess)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "job_vectors = vectorizer.fit_transform(df['clean'])\n",
    "\n",
    "# File Upload Widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e8f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e3521fa5944fbb86436e6d20005400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.txt', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ File Name: sample_resume.txt\n",
      "\n",
      "ðŸ“‘ Resume Content:\n",
      "\n",
      "I am experienced in Python programming, data analysis, and have worked on machine learning models using scikit-learn and TensorFlow. I also have exposure to cloud technologies like AWS.\n"
     ]
    }
   ],
   "source": [
    "upload_widget = widgets.FileUpload(accept='.txt', multiple=False)\n",
    "display(upload_widget)\n",
    "\n",
    "resume_text = \"\"\n",
    "\n",
    "def read_uploaded_file(change):\n",
    "    global resume_text\n",
    "    for filename, fileinfo in upload_widget.value.items():\n",
    "        resume_text = fileinfo['content'].decode('utf-8', errors='ignore')\n",
    "        print(f\"File Name: {filename}\\n\")\n",
    "        print(\"Resume Content:\\n\")\n",
    "        print(resume_text)\n",
    "\n",
    "upload_widget.observe(read_uploaded_file, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4668ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Job Matches:\n",
      "\n",
      " Job Title: Data Scientist\n",
      " Match Score: 0.46\n",
      " Description: Machine Learning Engineers develop machine learning models and algorithms, working on tasks like data preprocessing, model training, and deployment....\n",
      "\n",
      " Job Title: Data Scientist\n",
      " Match Score: 0.46\n",
      " Description: Machine Learning Engineers develop machine learning models and algorithms, working on tasks like data preprocessing, model training, and deployment....\n",
      "\n",
      " Job Title: Data Scientist\n",
      " Match Score: 0.46\n",
      " Description: Machine Learning Engineers develop machine learning models and algorithms, working on tasks like data preprocessing, model training, and deployment....\n",
      "\n",
      " Job Title: Data Scientist\n",
      " Match Score: 0.46\n",
      " Description: Machine Learning Engineers develop machine learning models and algorithms, working on tasks like data preprocessing, model training, and deployment....\n",
      "\n",
      " Job Title: Data Scientist\n",
      " Match Score: 0.46\n",
      " Description: Machine Learning Engineers develop machine learning models and algorithms, working on tasks like data preprocessing, model training, and deployment....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resume Matching\n",
    "if resume_text.strip():\n",
    "    resume_clean = preprocess(resume_text)\n",
    "    resume_vec = vectorizer.transform([resume_clean])\n",
    "    \n",
    "    sim_scores = cosine_similarity(resume_vec, job_vectors).flatten()\n",
    "    df['similarity'] = sim_scores\n",
    "\n",
    "    top_matches = df.sort_values(by='similarity', ascending=False).head(5)\n",
    "\n",
    "    print(\"\\nTop 5 Job Matches:\\n\")\n",
    "    for _, row in top_matches.iterrows():\n",
    "        print(f\" Job Title: {row['Job Title']}\")\n",
    "        print(f\" Match Score: {row['similarity']:.2f}\")\n",
    "        print(f\" Description: {row['Job Description'][:300]}...\\n\")\n",
    "else:\n",
    "    print(\" No resume uploaded or empty file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
